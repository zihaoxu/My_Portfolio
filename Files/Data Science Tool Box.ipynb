{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important packages to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os\n",
    "\n",
    "from sklearn import preprocessing, cross_validation, svm\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier # Load scikit's random forest classifier library\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Summary and inital cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# holistic summary of the given data set. \n",
    "# \"remove_bad_rowCol\" can be turned on to remove non-informative col / row\n",
    "def holistic_summary(df, remove_bad_rowCol = False, verbose = True):\n",
    "    # remove non-informative columns\n",
    "    if(remove_bad_rowCol):\n",
    "        df = df.drop(df.columns[df.isnull().sum() >= .9 * len(df)], axis = 1)\n",
    "        df = df.drop(df.index[df.isnull().sum(axis = 1) >= .5* len(df.columns)], axis = 0)\n",
    "        \n",
    "    # fix column names:\n",
    "    df.columns = [c.replace(\" \", \"_\").lower() for c in df.columns]\n",
    "    \n",
    "    print('***************************************************************')\n",
    "    print('Begin holistic summary: ')\n",
    "    print('***************************************************************\\n')\n",
    "    \n",
    "    print('Dimension of df: ' + str(df.shape))\n",
    "    print('Percentage of good observations: ' + str(1 - df.isnull().any(axis = 1).sum()/len(df)))\n",
    "    print('---------------------------------------------------------------\\n')\n",
    "    \n",
    "    print(\"Rows with nan values: \" + str(df.isnull().any(axis = 1).sum()))\n",
    "    print(\"Cols with nan values: \" + str(df.isnull().any(axis = 0).sum()))\n",
    "    print('Breakdown:')\n",
    "    print(df.isnull().sum()[df.isnull().sum()!=0])\n",
    "    print('---------------------------------------------------------------\\n')\n",
    "    \n",
    "    print('Columns details: ')\n",
    "    print('Columns with known dtypes: ')\n",
    "    good_cols = pd.DataFrame(df.dtypes[df.dtypes!='object'], columns = ['type'])\n",
    "    good_cols['nan_num'] = [df[col].isnull().sum() for col in good_cols.index]\n",
    "    good_cols['unique_val'] = [df[col].nunique() for col in good_cols.index]\n",
    "    good_cols['example'] = [df[col][1] for col in good_cols.index]\n",
    "    good_cols = good_cols.reindex(good_cols['type'].astype(str).str.len().sort_values().index)\n",
    "    print(good_cols)\n",
    "    print('\\n')\n",
    "    \n",
    "    try:\n",
    "        print('Columns with unknown dtypes:')\n",
    "        bad_cols = pd.DataFrame(df.dtypes[df.dtypes=='object'], columns = ['type'])\n",
    "        bad_cols['nan_num'] = [df[col].isnull().sum() for col in bad_cols.index]\n",
    "        bad_cols['unique_val'] = [df[col].nunique() for col in bad_cols.index]\n",
    "        bad_cols['example(sliced)'] = [str(df[col][1])[:10] for col in bad_cols.index]\n",
    "        bad_cols = bad_cols.reindex(bad_cols['example(sliced)'].str.len().sort_values().index)\n",
    "        print(bad_cols)\n",
    "    except Exception as e:\n",
    "        print('No columns with unknown dtypes!')\n",
    "    print('_______________________________________________________________\\n\\n\\n')\n",
    "    #if not verbose: enablePrint()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame cleaning and type fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixing dtypes: time and numeric variables\n",
    "def fix_dtypes(df, time_cols, num_cols):\n",
    "    \n",
    "    print('***************************************************************')\n",
    "    print('Begin fixing data types: ')\n",
    "    print('***************************************************************\\n')\n",
    "    \n",
    "    def fix_time_col(df, time_cols):\n",
    "        for time_col in time_cols:\n",
    "            df[time_col] = pd.to_datetime(df[time_col], errors = 'coerce')\n",
    "        print('---------------------------------------------------------------')\n",
    "        print('The following time columns has been fixed: ')\n",
    "        print(time_cols)\n",
    "        print('---------------------------------------------------------------\\n')\n",
    "\n",
    "    def fix_factor_col(df):\n",
    "        categorical_col = []\n",
    "        for col in df.columns:\n",
    "            if (df[col].nunique()<12) & (df[col].nunique()<len(df)/10):\n",
    "                df[col] = df[col].astype('category')\n",
    "                categorical_col.append(col)\n",
    "        print('---------------------------------------------------------------')\n",
    "        print('The following category columns has been fixed: ')\n",
    "        print(categorical_col)\n",
    "        print('---------------------------------------------------------------\\n')\n",
    "\n",
    "    def fix_num_col(df, num_cols):\n",
    "        for col in num_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors = 'coerce')\n",
    "        print('---------------------------------------------------------------')\n",
    "        print('The following number columns has been fixed: ')\n",
    "        print(num_cols)\n",
    "        print('---------------------------------------------------------------\\n')\n",
    "        \n",
    "    if(len(num_cols) > 0):\n",
    "        fix_num_col(df, num_cols)\n",
    "    fix_time_col(df, time_cols)\n",
    "    #fix_factor_col(df)\n",
    "    #fix_string_col(df)\n",
    "    print('---------------------------------------------------------------')\n",
    "    print('Final data types:')\n",
    "    result = pd.DataFrame(df.dtypes, columns = ['type'])\n",
    "    result = result.reindex(result['type'].astype(str).str.len().sort_values().index)\n",
    "    print(result)\n",
    "    print('_______________________________________________________________\\n\\n\\n')\n",
    "    return df\n",
    "\n",
    "# fix nan values \n",
    "def fix_nan(df, how = 'partial'):\n",
    "    \n",
    "    print('***************************************************************')\n",
    "    print('Begin fixing nans: ')\n",
    "    print('***************************************************************\\n')\n",
    "    if(how == 'partial'):     \n",
    "        print('The following columns are removed due to nan1: ')\n",
    "        print(df.columns[df.isnull().sum() >= .5 * len(df)])\n",
    "        df = df.drop(df.columns[df.isnull().sum() >= .5 * len(df)], axis = 1)\n",
    "    elif(how == 'all'):\n",
    "        print('The following columns are removed due to nan2: ')\n",
    "        print(df.columns[df.isnull().sum() == len(df)])\n",
    "        df = df.drop(df.columns[df.isnull().sum() == len(df)], axis = 1)\n",
    "    print('_______________________________________________________________\\n\\n\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation (clf, X_train, y_train, verbose = True, k = 5):\n",
    "    clf.fit(X_train, y_train) # fit the model\n",
    "    cv = KFold(X_train.shape[0], 5, shuffle = True)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv = cv) # 5-fold cross_validation\n",
    "    \n",
    "    if verbose == True: #if we want to print status\n",
    "        print(\"Coefficient of determination on training set:\", clf.score(X_train, y_train))\n",
    "        print(\"Average coefficient of determination using 5-fold crossvalidation:\", np.mean(scores))\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    # returns the mean score and 2 sigma\n",
    "    return np.mean(scores), scores.std() * 2\n",
    "\n",
    "def label_encode(df, col_encode):\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    for col in col_encode:\n",
    "        df[col] = encoder.fit_transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
